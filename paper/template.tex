\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2019}

\title{Expediting TTS Synthesis with Adversarial Vocoding}
\name{\text{*}Paarth Neekhara$^1$, \text{*}Chris Donahue$^{2}$, Miller Puckette$^2$, Shlomo Dubnov$^2$, Julian McAuley$^1$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$UC San Diego Department of Computer Science\\
  $^2$UC San Diego Department of Music\\
  \text{*} Equal contribution}
\email{pneekhar@eng.ucsd.edu, cdonahue@ucsd.edu}

\begin{document}

\maketitle
% 
\begin{abstract}
Recent approaches to text-to-speech (TTS) synthesis employ WaveNet to \emph{vocode} perceptually-informed spectrogram representations into listenable waveforms. 
Such an approach is computationally prohibitive in realistic TTS scenarios as they require generating audio one sample at a time. 
We propose an alternative approach which utilizes generative adversarial networks (GANs) to learn mappings from perceptually-informed spectrograms to simple magnitude spectrograms which can be vocoded to audio using principled strategies. 
Through a user study, we show that our approach is competitive with WaveNet-based methods for both real spectrograms and synthetic spectrograms generated by a TTS system, while being several orders of magnitude faster. 
Because our approach is independent of the TTS pipeline, it has the potential to benefit a number of applications where fast vocoding of spectrograms is desirable. 
As an example, we show that our approach can be used to effectively vocode spectrograms generated by a GAN, improving the state of the art in unsupervised audio generation.
\end{abstract}

%\noindent\textbf{Index Terms}: text-to-speech, vocoding, generative adversarial networks

\section{Introduction}

Generating natural-sounding speech from text is a well-studied problem with numerous potential applications. 
While past approaches were built on extensive engineering knowledge in the areas of linguistics and speech processing  (see \cite{} for a review), 
recent approaches adopt neural network strategies which learn from data to map linguistic representations into audio waveforms~\cite{arik2017deep,gibiansky2017deep,ping2017deep,wang2017tacotron,shen2018natural}. 
The two best performing of these systems~\cite{ping2017deep,shen2018natural}
are both comprised of two functional and independent mechanisms which 
1) map language into perceptually-informed spectrogram representations, and 
2) \emph{vocode} resultant spectrograms to listenable waveforms. 
In this work, we focus our efforts on the vocoding subproblem.

While the aforementioned TTS approaches differ substantially in their methods of mapping language to spectrograms, they both utilize the same method to vocode spectrograms into high-fidelity, natural-sounding waveforms. 
Specifically, they both utilize WaveNets~\cite{oord2016wavenet} conditioned on the synthetic spectrograms to synthesize audio. 
WaveNet is an autoregressive method which learns to predict individual audio samples given previous audio samples and---in this case---the desired spectrogram. 
Henceforth we will refer to this practice of using WaveNet to map spectrograms into waveforms as \emph{autoregressive vocoding}. 

Autoregressive vocoding is problematic as it necessitates running WaveNet once per individual audio sample (e.g. $16000$ times per second of audio), bottlenecking the overall TTS system as the language-to-spectrogram mechanisms are comparatively fast. 
While customized inference kernels can be used to expedite autoregressive vocoding~\cite{arik2017deep}, this approach is still an order of magnitude slower than alternative vocoding strategies which produce lower-fidelity results~\cite{ping2017deep}. 
Knowledge distillation~\cite{hinton2015distilling} strategies can also be used to transfer an autoregressive WaveNet to a model capable of parallel inference~\cite{oord2017parallel}, 
but this strategy is post-hoc and therefore cannot be trained end-to-end with the rest of the neural TTS system.

It is demonstrated in~\cite{ping2017deep} that autoregressive vocoding produces audio that is significantly higher quality than faster approaches to vocoding (which will be discussed in depth later in this paper). 
Thus autoregressive vocoding is central to the success of state-of-the-art TTS systems.
\textbf{This finding underlines a need for faster strategies for vocoding spectrograms to audio which do not compromise on audio quality.}

\subsection{Contributions}

\begin{itemize}
    \item hello
\end{itemize}

\section{Methodology}
Our goal is to invert the mel spectrogram feature representation into time domain waveform representation. 
Traditional heuristic based approaches achieve this goal using a two step process:
1) Estimating the magnitude spectrogram from the mel spectrogram using a linear transformation.
2) Phase information recovery from the estimated magnitude spectrogram using principled phase recovery methods like \cite{lws}. 
   
While this method is computationally very efficient, there is an \textit{observable degradation in the audio quality??} of the recovered waveform. We hypothesize, that the degradation in the quality of the recovered audio can be attributed to the first step of this recovery process. This hypothesis is based on the observation that the phase recovery methods \cite{lws}, work very well on \textit{actual} magnitude spectrograms of waveforms rather than the \textit{estimated} magnitude spectrograms. 
Since mel - spectrogram is a lossy compression of the magnitude spectrogram, a simple linear transformation for generating the magnitude spectrogram from mel spectrogram is an oversimplification of the recovery process. 
We model the problem of recovering magnitude spectrograms from the mel spectrogram as a generative modeling problem and propose a Generative Adversarial Networks based approach to solve it. 

% We observed that it is possible to recover the phase information very accurately using from the actual magnitude spectrogram of the waveform. We  
% linear transformation, and recover the phase information from the estimated magnitude spectrogram using principled phase recovery methods like \cite{lws}. 
% The degradation in quality of the waveform recovered using this approach can be attributed to the magnitude spectrogram 


% We observe that the phase recovery from magnitude magnitude spectrogram is 
% Since mel spectrogram is a lossy compression of the magnitude spectrogram, we argue that it is not not possible to recover the magnitude spectrogram accurately using a simple linear transformation. 

\subsubsection{Headings}

Section headings are centered in boldface with the first word capitalized and the rest of the heading in lower case. Sub- headings appear like major headings, except they start at the left margin in the column. Sub-sub-headings appear like sub-headings, except they are in italics and not boldface. See the examples in this file. No more than 3 levels of headings should be used.

\subsection{Equations}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figure.pdf}
  \caption{Schematic diagram of speech production.}
  \label{fig:speech_production}
\end{figure}

\begin{table}[t]
  \caption{Main predefined styles in Word}
  \label{tab:word_styles}
  \centering
  \begin{tabular}{ll}
    \toprule
    \textbf{Style Name}      & \textbf{Entities in a Paper}                \\
    \midrule
    Title                    & Title                                       \\
    Author                   & Author name                                 \\
    \textbullet\ List Bullet & Bulleted lists                              \\\relax
    [1] Reference            & References                                  \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Hyperlinks}

For technical reasons, the proceedings editor will strip all active links from the papers during processing. Hyperlinks can be included in your paper, if written in full, e.\,g.\ ``http://www.foo.com/index.html''. The link text must be all black. 
Please make sure that they present no problems in printing to paper.

\subsection{References}

The reference format is the standard IEEE one. References should be numbered in order of appearance, for example \cite{Davis80-COP}, \cite{Rabiner89-ATO}, \cite[pp.\ 417--422]{Hastie09-TEO}, and \cite{YourName17-XXX}.

\section{Acknowledgements}

The authors would like to thank Bo Li and Miller Puckette for helpful discussions about this work. 
This research was supported by the UC San Diego Chancellorâ€™s Research Excellence Scholarship program.
Thanks to NVIDIA for GPU donations which were used in the preparation of this work.


\bibliographystyle{IEEEtran}

\bibliography{mybib}

% \begin{thebibliography}{9}
% \bibitem[1]{Davis80-COP}
%   S.\ B.\ Davis and P.\ Mermelstein,
%   ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%   \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
% \bibitem[2]{Rabiner89-ATO}
%   L.\ R.\ Rabiner,
%   ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%   \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
% \bibitem[3]{Hastie09-TEO}
%   T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%   \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%   New York: Springer, 2009.
% \bibitem[4]{YourName17-XXX}
%   F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%   ``Title of your INTERSPEECH 2019 publication,''
%   in \textit{Interspeech 2019 -- 20\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 15-19, Graz, Austria, Proceedings, Proceedings}, 2019, pp.~100--104.
% \end{thebibliography}

\end{document}

