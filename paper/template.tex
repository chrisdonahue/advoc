\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2019}

\title{Expediting TTS Synthesis with Adversarial Vocoding}
\name{\text{*}Paarth Neekhara$^1$, \text{*}Chris Donahue$^{2}$, Miller Puckette$^2$, Shlomo Dubnov$^2$, Julian McAuley$^1$}
%The maximum number of authors in the author list is twenty. If the number of contributing authors is more than twenty, they should be listed in a footnote or in acknowledgement section, as appropriate.
\address{
  $^1$UC San Diego Department of Computer Science\\
  $^2$UC San Diego Department of Music\\
  \text{*} Equal contribution}
\email{pneekhar@eng.ucsd.edu, cdonahue@ucsd.edu}

\begin{document}

\maketitle
% 
\begin{abstract}
Recent approaches to text-to-speech (TTS) synthesis employ autoregressive methods (i.e. WaveNet) to \emph{vocode} perceptually-informed spectrogram representations into listenable waveforms. 
Such approaches are computationally prohibitive in realistic TTS scenarios as they require generating audio one sample at a time. 
We propose an alternative approach to autoregressive vocoding which utilizes generative adversarial networks (GANs) to learn mappings from perceptually-informed spectrograms to simple magnitude spectrograms which can be vocoded to audio using principled strategies. 
Through a user study, we show that our approach is competitive with autoregressive vocoding for both real spectrograms and synthetic spectrograms generated by a TTS system, while being several orders of magnitude faster. 
Because our approach is independent of the TTS pipeline, it has the potential to benefit a number of applications where fast vocoding of spectrograms is desirable. 
As an example, we show that our approach can be used to effectively vocode spectrograms generated by a GAN, improving the state of the art in unsupervised audio generation.
\end{abstract}

%\noindent\textbf{Index Terms}: text-to-speech, vocoding, generative adversarial networks

\section{Introduction}

Recent approaches to text-to-speech (TTS) synthesis employ end-to-end neural network approaches which learn to map character-level representations of language directly into audio waveforms. 
While these systems are trained end-to-end, they are comprised of two functional and independent mechanisms which 
1) map language into perceptually-informed spectrograms, and 
2) \emph{vocode} resultant spectrograms to appropriate waveforms. 
For the vocoding procedure, these approaches utilize autoregressive methods which generate one audio sample at a time, resulting in high computational overhead. 

This template can be found on the conference website. Templates are provided for Microsoft Word\textregistered, and \LaTeX. However, we highly recommend using \LaTeX when preparing your submission. Information for full paper submission is available on the conference website.

\subsection{Contributions}

\begin{itemize}
    \item hello
\end{itemize}

\section{Methodology}
Our goal is to invert the mel spectrogram feature representation into time domain waveform representation. 
Traditional heuristic based approaches estimate the magnitude spectrogram from the mel spectrogram, and recover the phase information from the estimated magnitude spectogram using phase recovery methods like \cite{lws}. However, since mel spectrogram is a lossy compression of the magnitude spectrogram

\subsubsection{Headings}

Section headings are centered in boldface with the first word capitalized and the rest of the heading in lower case. Sub- headings appear like major headings, except they start at the left margin in the column. Sub-sub-headings appear like sub-headings, except they are in italics and not boldface. See the examples in this file. No more than 3 levels of headings should be used.

\subsection{Equations}

\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figure.pdf}
  \caption{Schematic diagram of speech production.}
  \label{fig:speech_production}
\end{figure}

\begin{table}[t]
  \caption{Main predefined styles in Word}
  \label{tab:word_styles}
  \centering
  \begin{tabular}{ll}
    \toprule
    \textbf{Style Name}      & \textbf{Entities in a Paper}                \\
    \midrule
    Title                    & Title                                       \\
    Author                   & Author name                                 \\
    \textbullet\ List Bullet & Bulleted lists                              \\\relax
    [1] Reference            & References                                  \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Hyperlinks}

For technical reasons, the proceedings editor will strip all active links from the papers during processing. Hyperlinks can be included in your paper, if written in full, e.\,g.\ ``http://www.foo.com/index.html''. The link text must be all black. 
Please make sure that they present no problems in printing to paper.

\subsection{References}

The reference format is the standard IEEE one. References should be numbered in order of appearance, for example \cite{Davis80-COP}, \cite{Rabiner89-ATO}, \cite[pp.\ 417--422]{Hastie09-TEO}, and \cite{YourName17-XXX}.

\section{Acknowledgements}

The authors would like to thank Bo Li and Miller Puckette for helpful discussions about this work. 
This research was supported by the UC San Diego Chancellorâ€™s Research Excellence Scholarship program.
Thanks to NVIDIA for GPU donations which were used in the preparation of this work.


\bibliographystyle{IEEEtran}

\bibliography{mybib}

% \begin{thebibliography}{9}
% \bibitem[1]{Davis80-COP}
%   S.\ B.\ Davis and P.\ Mermelstein,
%   ``Comparison of parametric representation for monosyllabic word recognition in continuously spoken sentences,''
%   \textit{IEEE Transactions on Acoustics, Speech and Signal Processing}, vol.~28, no.~4, pp.~357--366, 1980.
% \bibitem[2]{Rabiner89-ATO}
%   L.\ R.\ Rabiner,
%   ``A tutorial on hidden Markov models and selected applications in speech recognition,''
%   \textit{Proceedings of the IEEE}, vol.~77, no.~2, pp.~257-286, 1989.
% \bibitem[3]{Hastie09-TEO}
%   T.\ Hastie, R.\ Tibshirani, and J.\ Friedman,
%   \textit{The Elements of Statistical Learning -- Data Mining, Inference, and Prediction}.
%   New York: Springer, 2009.
% \bibitem[4]{YourName17-XXX}
%   F.\ Lastname1, F.\ Lastname2, and F.\ Lastname3,
%   ``Title of your INTERSPEECH 2019 publication,''
%   in \textit{Interspeech 2019 -- 20\textsuperscript{th} Annual Conference of the International Speech Communication Association, September 15-19, Graz, Austria, Proceedings, Proceedings}, 2019, pp.~100--104.
% \end{thebibliography}

\end{document}

